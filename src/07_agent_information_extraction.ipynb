{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\LENOVO\\anaconda3\\envs\\bp_digitalization\\Lib\\s\n",
      "[nltk_data]     ite-packages\\llama_index\\core\\_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import tiktoken\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import RateLimitError, APIStatusError\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "from llama_index.core import (\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core.query_pipeline import QueryPipeline, FnComponent\n",
    "from mimetypes import guess_type\n",
    "from pydantic import BaseModel, Field, conint, confloat\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from helpers.helpers import read_json_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from azure_authentication.customized_azure_login import CredentialFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "CWD = os.getcwd()\n",
    "\n",
    "data_dir = os.path.join(CWD, 'data')\n",
    "\n",
    "#Specify mode (working with a sample or all the files?)\n",
    "sample_mode = False \n",
    "sample_size = 50\n",
    "\n",
    "# specify file path\n",
    "INPUT_FILE_PATH = os.path.join(CWD, \"data\", \"proc\", \"building_plans_sample\", \"test_images\", \"bp_text.json\")\n",
    "METADATA_PATH = os.path.join(CWD, \"data\", \"proc\", \"building_plans\", \"metadata\",\"building_plans_metadata.csv\")\n",
    "\n",
    "# specify relevant column names\n",
    "ID_COLUMN='filename'\n",
    "TEXT_COLUMN='content'\n",
    "\n",
    "# read in data\n",
    "bp_text = pd.read_json(INPUT_FILE_PATH)\n",
    "metadata_df = pd.read_csv(METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_bps = metadata_df[metadata_df['Planart'].isin(['qualifizierter Bebauungsplan', 'einfacher Bebauungsplan', 'vorhabenbezogener Bebauungsplan'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_text['id'] = bp_text['filename'].str.extract(r'(\\d+)_').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_df = metadata_bps.merge(bp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = bp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate Azure OpenAI Client\n"
     ]
    }
   ],
   "source": [
    "# Recommendation: Configure your own authentication workflow with environment variables, see the description at\n",
    "# https://github.com/soda-lmu/azure-auth-helper-python/blob/main/AuthenticationWorkflowSetup.md\n",
    "credential = CredentialFactory().select_credential()\n",
    "token_provider = credential.get_login_token_to_azure_cognitive_services()\n",
    "\n",
    "print(\"Instantiate Azure OpenAI Client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llm:\n",
    "    \"\"\"\n",
    "\n",
    "    Contains LLM model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name=\"gpt-35-turbo-16k\",\n",
    "                 max_parallel_llm_prompts_running=8):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        \n",
    "        credential = CredentialFactory().select_credential()\n",
    "        token_provider = credential.get_login_token_to_azure_cognitive_services()\n",
    "\n",
    "        if self.model_name == \"gpt-35-turbo-16k\":\n",
    "\n",
    "            self.llamaindex_llm = AzureOpenAI(\n",
    "                engine=\"gpt-35-turbo-0301\",\n",
    "                model=\"gpt-35-turbo-16k\",\n",
    "                temperature=0.0,\n",
    "                azure_endpoint=os.environ[\"AZURE_ENDPOINT_GIST_PROJECT_WESTEUROPE\"],\n",
    "                # use_azure_ad=True, # only useful for debugging purposes?\n",
    "                api_version=\"2024-02-01\",\n",
    "                api_key=token_provider()\n",
    "            )\n",
    "\n",
    "            self.token_counter = TokenCountingHandler(\n",
    "                # both gpt-3.5-turbo and gpt-4 are based on the same cl100k_base encoding -> doesn't matter which model we use here\n",
    "                # tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "                tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    "            )\n",
    "\n",
    "            self.semaphore = asyncio.Semaphore(\n",
    "                max_parallel_llm_prompts_running)\n",
    "\n",
    "        elif self.model_name == \"gpt-4-1106-preview\":\n",
    "\n",
    "            self.llamaindex_llm = AzureOpenAI(\n",
    "                engine=\"gpt-4-1106-preview\", model=\"gpt-4-1106-preview\", temperature=0.0,\n",
    "                azure_endpoint=os.environ[\"AZURE_ENDPOINT_GIST_PROJECT_NORWAYEAST\"],\n",
    "                # use_azure_ad=True, # only useful for debugging purposes?\n",
    "                api_version=\"2024-02-01\",\n",
    "                api_key=token_provider(),\n",
    "                max_retries=4,\n",
    "                timeout=240.0,\n",
    "                reuse_client=False)\n",
    "\n",
    "            self.token_counter = TokenCountingHandler(\n",
    "                # both gpt-3.5-turbo and gpt-4 are based on the same cl100k_base encoding -> doesn't matter which model we use here\n",
    "                # tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "                tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    "            )\n",
    "\n",
    "            self.semaphore = asyncio.Semaphore(\n",
    "                max_parallel_llm_prompts_running)\n",
    "\n",
    "        Settings.llm = self.llamaindex_llm\n",
    "\n",
    "    def calculate_llm_calling_price(self, input_tokens, output_tokens):\n",
    "        \"\"\"\n",
    "        Cost calculator\n",
    "        based on prices from https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/#pricing\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_name == \"gpt-4-1106-preview\":\n",
    "            return input_tokens / 1000 * 0.010 + output_tokens / 1000 * 0.029\n",
    "        elif self.model_name == \"gpt-35-turbo-16k\":\n",
    "            return input_tokens / 1000 * 0.0005 + output_tokens / 1000 * 0.0015\n",
    "        else:\n",
    "            return -1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRZ(BaseModel):\n",
    "    value: Optional[float] = Field(\n",
    "        None,\n",
    "        description=\"Der numerische Wert der Grundflächenzahl oder 'null', falls nicht vorhanden.\",\n",
    "        example=0.75\n",
    "    )\n",
    "\n",
    "class GFZ(BaseModel):\n",
    "    value: Optional[float] = Field(\n",
    "        None,\n",
    "        description=\"Der numerische Wert der Geschoßflächenzahl oder 'null', falls nicht vorhanden.\",\n",
    "        example=1.0\n",
    "    )\n",
    "\n",
    "class BuildingMetrics(BaseModel):\n",
    "    \n",
    "    grz: Optional[GRZ] = Field(None, description=\"Grundflächenzahl (GRZ)\")\n",
    "    \n",
    "    gfz: Optional[GFZ] = Field(None, description=\"Geschoßflächenzahl (GFZ)\")\n",
    "    \n",
    "class PromptRoleAndTask:\n",
    "    \"\"\"Describes LLM role and task for prompt.\"\"\"\n",
    "\n",
    "    role: str = \"You are a helpful enviromental city planner. \\n Based on the excerpt from a building plan provided below, we would like to extract following information.\\n\"\n",
    "\n",
    "class PromptKpiDefinitions:\n",
    "    \"\"\"Provides definitions to each KPI in prompt.\"\"\"\n",
    "\n",
    "    definitions_string : str = read_json_to_str('./query/definitions.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Llm_Extraction_Prompt:\n",
    "    \"\"\"\n",
    "    The dataclass contains a prompt (=query text).\n",
    "    Strategy: We make a single query to extract relevant info from BP.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"The dataclass contains a prompt (=query text) and a parser method for this prompt.\n",
    "        Strategy: We make a single query to extract relevant info from BP.\n",
    "\"\"\"\n",
    "\n",
    "    role: Optional[str] = field(default=PromptRoleAndTask.role)\n",
    "    KPIDefinitions: Optional[str] = field(default=PromptKpiDefinitions().definitions_string)\n",
    "    #specifications: Optional[str] = field(default=prompt_specifications.specifications)\n",
    "\n",
    "    def __init__(self, role=None, KPIDefinitions=None#, specifications=None\n",
    "                 ):\n",
    "        \"\"\"Optional parameters allow default values to be loaded from a file if None is provided.\"\"\"\n",
    "        if role is None:\n",
    "            role = PromptRoleAndTask.role  # Default role definition\n",
    "        if KPIDefinitions is None:\n",
    "            KPIDefinitions = PromptKpiDefinitions().definitions_string  # Default KPI definitions\n",
    "#        if specifications is None:\n",
    "#            specifications = prompt_specifications.specifications  # Default specifications\n",
    "\n",
    "        self.query = f'{role}\\n{KPIDefinitions}\\n\\\n",
    "        Here is the excerpt: \\n {{context_str}}'\n",
    "\n",
    "        self.parser = PydanticOutputParser(output_cls=BuildingMetrics)\n",
    "\n",
    "    def parse_gpt_output(self, gpt_question_output) -> pd.DataFrame:\n",
    "        \"\"\"Extract year, scope, value, and unit gpt_question_output using regular expressions.\"\"\"\n",
    "\n",
    "        building_metrics = self.parser.parse(gpt_question_output.message.content)\n",
    "\n",
    "        #parsed_output = pd.DataFrame([entry.dict()\n",
    "        #                             for entry in building_metrics.BuildingMetrics])\n",
    "\n",
    "        return(building_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BP_Metrics_Getter:\n",
    "    \"\"\"Retrieve emissions from a single document.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, llm_single_prompt):\n",
    "        self.llamaindex_llm = llm.llamaindex_llm\n",
    "        self.llm_semaphore = llm.semaphore\n",
    "        self.token_counter = llm.token_counter\n",
    "        self.llm_single_prompt = llm_single_prompt\n",
    "        self.start_time = time.perf_counter()\n",
    "\n",
    "    async def _bound_get_emissions_from_raw_text(self, doc_text):\n",
    "        \"\"\"Getter function with semaphore.\"\"\"\n",
    "\n",
    "        async with self.llm_semaphore:\n",
    "            current_time = datetime.now()\n",
    "            cur_time = time.perf_counter()\n",
    "            elapsed_time = cur_time - self.start_time\n",
    "            print(\"Current Time:\", current_time.strftime(\"%H:%M:%S\"), \"(\", elapsed_time / 60,\n",
    "                  \"minutes since process started.)\")\n",
    "            \n",
    "            res = await self._run_llm(doc_text)\n",
    "\n",
    "            duration_time = time.perf_counter() - cur_time\n",
    "            total_duration_time = time.perf_counter() - self.start_time\n",
    "            print(\"LLM query execution time: \" + str(duration_time) + \" seconds (\" +\n",
    "                  str(total_duration_time) + \" seconds since initiating ValueRetrieverPipeline).\")\n",
    "        return res\n",
    "\n",
    "\n",
    "    async def _run_llm(self, doc_text: str):\n",
    "        \"\"\"Extract emissions from a single page of a document.\n",
    "\n",
    "        Example code to extract emissions from a single PDF document\n",
    "\n",
    "        embed_model = config.EmbeddingModel().embed_model\n",
    "        llm = config.Llm(model_name=\"gpt-35-turbo-16k\")\n",
    "        llm_single_prompt = prompts_with_prompt_parsers.LlmSinglePromptQueryScope123()\n",
    "        start_time = time.perf_counter()\n",
    "        emission_getter = EmissionsGetter(\n",
    "            llm=llm, llm_single_prompt=llm_single_prompt, start_time=start_time)\n",
    "\n",
    "        pathname_list = glob.glob(os.path.join(\n",
    "            path_to_data, \"input-data/*.pdf\"))\n",
    "        doc = semantic_search.Pdfdoc(filename=filename)\n",
    "        relevant_pages = doc.retrieve_relevant_pages(embed_model)\n",
    "\n",
    "        output = await asyncio.gather(*(emission_getter.get_emissions_from_raw_text(doc_text=page.text) for page in relevant_pages))\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = self.llm_single_prompt.query\n",
    "        parsing_instruction = self.llm_single_prompt.parser\n",
    "        prompt_tmpl = PromptTemplate(template=f\"{prompt}\",\n",
    "                                     output_parser=parsing_instruction)\n",
    "\n",
    "        p = QueryPipeline(modules={\"llm_prompt\": prompt_tmpl,\n",
    "                                   \"llm\": self.llamaindex_llm},\n",
    "                          verbose=False)\n",
    "        \n",
    "        p.add_chain([\"llm_prompt\", \"llm\"])\n",
    "\n",
    "        try:\n",
    "            results = await p.arun_multi({\"llm_prompt\": {\"context_str\": doc_text}})\n",
    "                                         \n",
    "            raw_response = results[\"llm\"][\"output\"]\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            print(\n",
    "                \"A 429 status code (Rate Limit error) was received; we should back off a bit.\")\n",
    "            raw_response = \"Create empty output table\"\n",
    "\n",
    "        except APIStatusError as e:\n",
    "\n",
    "            raw_response = \"Create empty output table\"\n",
    "\n",
    "            print(\"Another non-200-range status code was received\")\n",
    "            print(e.status_code)\n",
    "            print(e.response)\n",
    "\n",
    "        return raw_response\n",
    "    \n",
    "    def _parse_to_table_llm_output(self, llm_output):\n",
    "\n",
    "        output_string = str(llm_output)\n",
    "\n",
    "        parsed_output = self.llm_single_prompt.parse_gpt_output(output_string)\n",
    "\n",
    "        return parsed_output\n",
    "\n",
    "    def _parse_to_classes_llm_output(self, llm_output):\n",
    "\n",
    "        parsed_output = self.llm_single_prompt.parse_gpt_output(llm_output)\n",
    "\n",
    "        return parsed_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_mode:\n",
    "    \n",
    "    run_data = input_df.sample(sample_size, random_state=15)\n",
    "\n",
    "else: \n",
    " \n",
    "    run_data = input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llm(model_name=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = BP_Metrics_Getter(llm = llm, llm_single_prompt = Llm_Extraction_Prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time: 10:33:45 ( 0.05705804166694482 minutes since process started.)\n",
      "LLM query execution time: 5.4869639000389725 seconds (8.910447800066322 seconds since initiating ValueRetrieverPipeline).\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "extraction_results = await getter._bound_get_emissions_from_raw_text(run_data['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_extractions = getter._parse_to_classes_llm_output(extraction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'tuple'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extraction_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_extractions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\bp_digitalization\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\bp_digitalization\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:448\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m sample, objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_object(objs, ndims, keys, names, levels)\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\bp_digitalization\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:489\u001b[0m, in \u001b[0;36m_Concatenator._get_ndims\u001b[1;34m(self, objs)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[0;32m    485\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    488\u001b[0m         )\n\u001b[1;32m--> 489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    491\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'tuple'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "extraction_df = pd.concat(parsed_extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FLOODING_FILE_PATH = os.path.join(\"data\", \"proc\", \"building_plans_sample\", \"features\",  \"test_images_flooding_data_extraction.csv\")\n",
    "OUTPUT_EXTRACTIONS_FILE_PATH = os.path.join(\"data\", \"proc\", \"building_plans_sample\", \"features\",  \"test_images_info_data_extraction.csv\")\n",
    "\n",
    "\n",
    "extraction_df.to_csv(OUTPUT_EXTRACTIONS_FILE_PATH)\n",
    "flooding_df.to_csv(OUTPUT_FLOODING_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp_digitalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
