{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\land-sealing-dssgx\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import tiktoken\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import RateLimitError, APIStatusError\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "from llama_index.core import (\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core.query_pipeline import QueryPipeline, FnComponent\n",
    "from mimetypes import guess_type\n",
    "\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "- Improve prompt.\n",
    "- Run on all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "CWD = os.getcwd()\n",
    "\n",
    "data_dir = os.path.join(CWD, 'data')\n",
    "\n",
    "#Specify mode (working with a sample or all the files?)\n",
    "sample_mode = True \n",
    "sample_size = 20\n",
    "\n",
    "# specify file path\n",
    "INPUT_FILE_PATH = os.path.join(\"data\", \"proc\", \"building_plans\", \"text\", \"bp_text.json\")\n",
    "METADATA_PATH = os.path.join(\"data\", \"proc\", \"building_plans\", \"metadata\",\"building_plans_metadata.csv\")\n",
    "\n",
    "# specify relevant column names\n",
    "ID_COLUMN='filename'\n",
    "TEXT_COLUMN='content'\n",
    "\n",
    "# read in data\n",
    "bp_text = pd.read_json(INPUT_FILE_PATH)\n",
    "metadata_df = pd.read_csv(METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_bps = metadata_df[metadata_df['Planart'].isin(['qualifizierter Bebauungsplan', 'einfacher Bebauungsplan', 'vorhabenbezogener Bebauungsplan'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_text['id'] = bp_text['filename'].str.extract(r'(\\d+)_').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = metadata_bps.merge(bp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use httpimport to load 'azure_authentication' package remotely from GitHub without installing it\n",
    "import httpimport\n",
    "with httpimport.remote_repo('https://raw.githubusercontent.com/soda-lmu/azure-auth-helper-python/main/src'\n",
    "                            '/azure_authentication/'):\n",
    "    from customized_azure_login import CredentialFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate Azure OpenAI Client\n"
     ]
    }
   ],
   "source": [
    "# Recommendation: Configure your own authentication workflow with environment variables, see the description at\n",
    "# https://github.com/soda-lmu/azure-auth-helper-python/blob/main/AuthenticationWorkflowSetup.md\n",
    "credential = CredentialFactory().select_credential()\n",
    "token_provider = credential.get_login_token_to_azure_cognitive_services()\n",
    "\n",
    "print(\"Instantiate Azure OpenAI Client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm4 = AzureOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    engine=\"gpt-4-1106-preview\", \n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    api_key=token_provider(),  # alternative: insert os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    api_version=\"2024-02-01\",  # or use a preview version (e.g., \"2024-03-01-preview\") for the latest features.\n",
    "    # api_version (How-To): https://stackoverflow.com/questions/76475419/how-can-i-select-the-proper-openai-api-version\n",
    "    timeout=600.0,  # throw APITimeoutError after 10 minutes without a response (default behavior)\n",
    ")\n",
    "\n",
    "#MODEL_DEPLOYMENT_NAME = \"gpt-4-turbo-vision-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "\n",
    "class Llm_Extraction_Prompt:\n",
    "    \"\"\"\n",
    "    The dataclass contains a prompt (=query text).\n",
    "    Strategy: We make a single query to extract relevant info from BP.\n",
    "    \"\"\"\n",
    "    query: str = (\n",
    "        \"You are a helpful enviromental city planner.\" #city urban planner / spatial planner\n",
    "        \"Based on the excerpt from a building plan provided below, we would like to extract following information.\\n\"\n",
    "        \"1. Maximal construction height (GFZ): numeric value and unit of measurement.  \\n\"\n",
    "        \"2. Maximal floor coverage (GRZ): numeric value and unit of measurement.\\n\" #Add unit of measurement\n",
    "        \"3. Types of building use (Art der baulichen Nutzung): list all that appear.\"\n",
    "        \"4. Appearance of green areas (Grünflächen): True/False value.\" \n",
    "        \"5. Firsthöhe length (FH): numeric value and unit of measurement.\"\n",
    "        \"6. Traufhöhe length (TH): numeric value and unit of measurement.\"\n",
    "        \"7. Company names mentioned: list all that appear.\"\n",
    "        \"If a particular piece of information is not present, output 'Not specified'.\\n\\n\"\n",
    "        \"Extract the information into appropiate JSON.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"\\n\\nHere is the excerpt:\\n {context_str}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_gpt_output(gpt_question_output) -> pd.DataFrame:\n",
    "        \"\"\"Extract year, scope, value, and unit gpt_question_output using regular expressions.\"\"\"\n",
    "\n",
    "        pattern = r'```json\\n({.*?})\\n```'\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        match = re.search(pattern, gpt_question_output, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            #json_str = match['output_parser']['res'].group(1)\n",
    "            json_str = match.group(1)\n",
    "            json_dict = json.loads(json_str)\n",
    "            rows.append(json_dict)\n",
    "    \n",
    "\n",
    "        output_table = pd.DataFrame(rows)\n",
    "        output_table[\"raw_llm_response\"] = gpt_question_output\n",
    "\n",
    "        return(output_table)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "\n",
    "class Llm_Flooding_Prompt:\n",
    "    \"\"\"\n",
    "    The dataclass contains a prompt (=query text).\n",
    "    Strategy: We make a single query to extract relevant info from BP.\n",
    "    \"\"\"\n",
    "    query: str = (\n",
    "        \"You are a helpful environmental city planner. \"\n",
    "        \"Based on the excerpt from a building plan provided below, we would like to know if the building has taken measures against flooding risk.\\n\"\n",
    "        \"1. Identify if there are water bodies mentioned in the text: list all that appear. \\n\"\n",
    "        \"2. Flooding risk prevention measures: list all that appear.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Present the output in a JSON format where the keys are 'water_bodies', 'flooding_risk_prevention_measures' and the values are a list with the results.\"\n",
    "        \"---------------------\\n\"\n",
    "        \"\\n\\nHere is the excerpt:\\n {context_str}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_gpt_output(gpt_question_output) -> pd.DataFrame:\n",
    "        \"\"\"Extract year, scope, value, and unit gpt_question_output using regular expressions.\"\"\"\n",
    "\n",
    "        pattern = r'```json\\n({.*?})\\n```'\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        match = re.search(pattern, gpt_question_output, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            json_dict = json.loads(json_str)\n",
    "            rows.append(json_dict)\n",
    "    \n",
    "\n",
    "        output_table = pd.DataFrame(rows)\n",
    "        output_table[\"raw_llm_response\"] = gpt_question_output\n",
    "\n",
    "        return(output_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_pipeline_query(prompt_selected):\n",
    "\n",
    "    prompt_tmpl = PromptTemplate(prompt_selected.query)\n",
    "    string_converter = FnComponent(fn=lambda x: str(x), output_key=\"gpt_question_output\")\n",
    "    output_parser = FnComponent(fn=prompt_selected.parse_gpt_output, output_key=\"res\")\n",
    "\n",
    "    p = QueryPipeline(modules={\"llm_prompt\": prompt_tmpl,\n",
    "                            \"llm\": llm4,\n",
    "                            \"string_converter\": string_converter,\n",
    "                            \"output_parser\": output_parser\n",
    "                            },\n",
    "                            verbose=False)\n",
    "    p.add_chain([\"llm_prompt\", \"llm\", \"string_converter\", \"output_parser\"])\n",
    "    p.add_link( \"string_converter\", \"output_parser\")\n",
    "\n",
    "    return(p)\n",
    "\n",
    "async def run_pipeline_on_rows(str_text, \n",
    "                         pipeline):\n",
    "\n",
    "    res = await pipeline.arun_multi({'llm_prompt':{'context_str' : str_text}})\n",
    "\n",
    "    return(res)\n",
    "\n",
    "async def extract_bp_info(prompt_selected,\n",
    "                    data):\n",
    "    \n",
    "    p = define_pipeline_query(prompt_selected)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        res = await run_pipeline_on_rows(row['content'], p)\n",
    "        table_output = res['output_parser']['res']\n",
    "        table_output['id'] = row['id']\n",
    "        table_output['filename'] = row['filename']\n",
    "        results.append(table_output)\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_mode: \n",
    "    \n",
    "    run_data = input_df.sample(sample_size, random_state=15)\n",
    "\n",
    "else: \n",
    "\n",
    "    run_data = input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "flooding_results = await extract_bp_info(Llm_Flooding_Prompt(), run_data)\n",
    "extraction_results = await extract_bp_info(Llm_Extraction_Prompt(), run_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_df = pd.concat(extraction_results)\n",
    "flooding_df = pd.concat(flooding_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FLOODING_FILE_PATH = os.path.join(\"data\", \"proc\", \"building_plans_sample\", \"features\",  \"flooding_data_extraction.csv\")\n",
    "OUTPUT_EXTRACTIONS_FILE_PATH = os.path.join(\"data\", \"proc\", \"building_plans_sample\", \"features\",  \"info_data_extraction.csv\")\n",
    "\n",
    "\n",
    "extraction_df.to_csv(OUTPUT_EXTRACTIONS_FILE_PATH)\n",
    "flooding_df.to_csv(OUTPUT_FLOODING_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "land-sealing-dssgx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
